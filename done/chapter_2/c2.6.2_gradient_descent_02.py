# import thÆ° viá»‡n 
import numpy as np
import matplotlib.pyplot as plt

# 1. Äá»‹nh nghÄ©a hÃ m loss
# Giáº£ sá»­ mÃ¬nh cÃ³ hÃ m loss ğ“›(Î¸) = Î¸Â² - 8Î¸ + 10
# khai bÃ¡o hÃ m loss
def cal_loss_function(theta):
    return theta**2 - 8*theta + 10

# Äáº¡o hÃ m cá»§a hÃ m Loss ğ“›'(Î¸) = 2*Î¸ - 8 (Ä‘áº¡o hÃ m cá»§a Î¸Â² - 8Î¸ + 10))
def cal_loss_derivative(theta):
    return 2*theta - 8

# 2. Váº½ Ä‘á»“ thá»‹ hÃ m loss
#Khai bÃ¡o khoáº£ng giÃ¡ trá»‹ cá»§a theta
theta_values = np.linspace(-4, 12, 400)

#giÃ¡ trá»‹ cá»§a hÃ m loss theo khoáº£ng theta
loss_values = cal_loss_function(theta_values)

#Váº½ Ä‘á»“ thá»‹ hÃ m loss
plt.plot(theta_values, loss_values)

# 3. Khá»Ÿi táº¡o
# Khá»Ÿi táº¡o giÃ¡ trá»‹ ban Ä‘áº§u cá»§a theta 
theta = 11 
# Khá»Ÿi táº¡o siÃªu tham sá»‘ - hyperparameters
learning_rate = 0.1
# epsilon = 1e-4 
N = 100 # sá»‘ vÃ²ng láº·p

# cÃ¡ch 2: Khai bÃ¡o sá»‘ vÃ²ng láº·p
for i in range(N):
    #tÃ­nh giÃ¡ trá»‹ hÃ m loss
    loss = cal_loss_function(theta)

    #tÃ­nh giÃ¡ trá»‹ Ä‘áº¡o hÃ m cá»§a hÃ m loss theo theta
    grad = cal_loss_derivative(theta)

    # Váº½ Ä‘iá»ƒm theta hiá»‡n táº¡i 
    plt.plot(theta, loss, 'ro') # ro lÃ  Ä‘iá»ƒm trÃ²n mÃ u Ä‘á»

    #Dá»«ng Ä‘á»ƒ quan sÃ¡t
    plt.pause(0.1)

    # Cáº­p nháº­t theta
    theta = theta - learning_rate * grad


plt.show()
print("GiÃ¡ trá»‹ theta tá»‘i Æ°u::::", theta)


